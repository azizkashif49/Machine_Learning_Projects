{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76LJP1Qk1A7t"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install transformers[sentencepiece] datasets sacrebleu rouge_score py7zr -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ugl3BIpp1MSC",
        "outputId": "446c8c96-5df6-4461-c293-8cea7dc164c4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline, set_seed\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "from datasets import load_dataset, load_metric\n",
        "\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "nltk.download(\"punkt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFENua4e1zyA",
        "outputId": "2caa170a-ebac-41d4-a9d4-43966d124c90"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.encoder.embed_positions.weight', 'model.decoder.embed_positions.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model_ckpt = \"google/pegasus-cnn_dailymail\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
        "\n",
        "model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Ba2Gwnp2BFz"
      },
      "outputs": [],
      "source": [
        "def generate_batch_sized_chunks(list_of_elements, batch_size):\n",
        "    \"\"\"split the dataset into smaller batches that we can process simultaneously\n",
        "    Yield successive batch-sized chunks from list_of_elements.\"\"\"\n",
        "    for i in range(0, len(list_of_elements), batch_size):\n",
        "        yield list_of_elements[i : i + batch_size]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "taRzThrB2ZIV"
      },
      "outputs": [],
      "source": [
        "\n",
        "def calculate_metric_on_test_ds(dataset, metric, model, tokenizer,\n",
        "                               batch_size=16, device=device,\n",
        "                               column_text=\"article\",\n",
        "                               column_summary=\"highlights\"):\n",
        "    article_batches = list(generate_batch_sized_chunks(dataset[column_text], batch_size))\n",
        "    target_batches = list(generate_batch_sized_chunks(dataset[column_summary], batch_size))\n",
        "\n",
        "    for article_batch, target_batch in tqdm(\n",
        "        zip(article_batches, target_batches), total=len(article_batches)):\n",
        "\n",
        "        inputs = tokenizer(article_batch, max_length=1024,  truncation=True,\n",
        "                        padding=\"max_length\", return_tensors=\"pt\")\n",
        "\n",
        "        summaries = model.generate(input_ids=inputs[\"input_ids\"].to(device),\n",
        "                         attention_mask=inputs[\"attention_mask\"].to(device),\n",
        "                         length_penalty=0.8, num_beams=8, max_length=128)\n",
        "        ''' parameter for length penalty ensures that the model does not generate sequences that are too long. '''\n",
        "\n",
        "        # Finally, we decode the generated texts,\n",
        "        # replace the  token, and add the decoded texts with the references to the metric.\n",
        "        decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True,\n",
        "                                clean_up_tokenization_spaces=True)\n",
        "               for s in summaries]\n",
        "\n",
        "        decoded_summaries = [d.replace(\"\", \" \") for d in decoded_summaries]\n",
        "\n",
        "\n",
        "        metric.add_batch(predictions=decoded_summaries, references=target_batch)\n",
        "\n",
        "    #  Finally compute and return the ROUGE scores.\n",
        "    score = metric.compute()\n",
        "    return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6h1zj5Ew23YJ",
        "outputId": "a01e4e82-dff6-4563-881d-d48442be70ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Split lengths: [14732, 819, 818]\n",
            "Features: ['id', 'dialogue', 'summary']\n",
            "\n",
            "Dialogue:\n",
            "Eric: MACHINE!\r\n",
            "Rob: That's so gr8!\r\n",
            "Eric: I know! And shows how Americans see Russian ;)\r\n",
            "Rob: And it's really funny!\r\n",
            "Eric: I know! I especially like the train part!\r\n",
            "Rob: Hahaha! No one talks to the machine like that!\r\n",
            "Eric: Is this his only stand-up?\r\n",
            "Rob: Idk. I'll check.\r\n",
            "Eric: Sure.\r\n",
            "Rob: Turns out no! There are some of his stand-ups on youtube.\r\n",
            "Eric: Gr8! I'll watch them now!\r\n",
            "Rob: Me too!\r\n",
            "Eric: MACHINE!\r\n",
            "Rob: MACHINE!\r\n",
            "Eric: TTYL?\r\n",
            "Rob: Sure :)\n",
            "\n",
            "Summary:\n",
            "Eric and Rob are going to watch a stand-up on youtube.\n"
          ]
        }
      ],
      "source": [
        "dataset_samsum = load_dataset(\"samsum\")\n",
        "\n",
        "split_lengths = [len(dataset_samsum[split])for split in dataset_samsum]\n",
        "\n",
        "print(f\"Split lengths: {split_lengths}\")\n",
        "print(f\"Features: {dataset_samsum['train'].column_names}\")\n",
        "print(\"\\nDialogue:\")\n",
        "\n",
        "print(dataset_samsum[\"test\"][1][\"dialogue\"])\n",
        "\n",
        "print(\"\\nSummary:\")\n",
        "\n",
        "print(dataset_samsum[\"test\"][1][\"summary\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yC-__tMU3Wp9"
      },
      "source": [
        "# **Evaluating PEGASUS on SAMSum**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "_n0NmkOf23tD",
        "outputId": "c241e2f9-a519-464c-b797-e1dd3c0e8b15"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Hannah: Hey, do you have Betty's number?\\nAmanda: Lemme check\\nHannah: <file_gif>\\nAmanda: Sorry, can't find it.\\nAmanda: Ask Larry\\nAmanda: He called her last time we were at the park together\\nHannah: I don't know him well\\nHannah: <file_gif>\\nAmanda: Don't be shy, he's very nice\\nHannah: If you say so..\\nHannah: I'd rather you texted him\\nAmanda: Just text him ðŸ™‚\\nHannah: Urgh.. Alright\\nHannah: Bye\\nAmanda: Bye bye\""
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "dataset_samsum['test'][0]['dialogue']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-nqRzLl3aKq",
        "outputId": "1fd72078-0b06-4c19-fdbb-42a49c1e806d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.encoder.embed_positions.weight', 'model.decoder.embed_positions.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Your max_length is set to 128, but your input_length is only 122. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=61)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "pipe = pipeline('summarization', model = model_ckpt )\n",
        "\n",
        "pipe_out = pipe(dataset_samsum['test'][0]['dialogue'] )\n",
        "\n",
        "print(pipe_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "arqgQ21j3c3h",
        "outputId": "5f2f2070-0578-42d3-8b5e-7ef3919a0d05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Amanda: Ask Larry Amanda: He called her last time we were at the park together.\n",
            "<n>Hannah: I'd rather you texted him.\n",
            "<n>Amanda: Just text him.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(pipe_out[0]['summary_text'].replace(\" .\", \".\\n\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_DQM2lsC3fqQ",
        "outputId": "12f7a7b5-818d-443f-cea3-90d78db9285b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/103 [00:02<?, ?it/s]\n"
          ]
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-388971e09d3a>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrouge_metric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rouge'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_metric_on_test_ds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_samsum\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrouge_metric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_pegasus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'dialogue'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_summary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'summary'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-21-fd0295f626ab>\u001b[0m in \u001b[0;36mcalculate_metric_on_test_ds\u001b[0;34m(dataset, metric, model, tokenizer, batch_size, device, column_text, column_summary)\u001b[0m\n\u001b[1;32m     12\u001b[0m                         padding=\"max_length\", return_tensors=\"pt\")\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         summaries = model.generate(input_ids=inputs[\"input_ids\"].to(device),\n\u001b[0m\u001b[1;32m     15\u001b[0m                          \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"attention_mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                          length_penalty=0.8, num_beams=8, max_length=128)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1750\u001b[0m             )\n\u001b[1;32m   1751\u001b[0m             \u001b[0;31m# 13. run beam search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1752\u001b[0;31m             return self.beam_search(\n\u001b[0m\u001b[1;32m   1753\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1754\u001b[0m                 \u001b[0mbeam_scorer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mbeam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   3089\u001b[0m             \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_inputs_for_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3091\u001b[0;31m             outputs = self(\n\u001b[0m\u001b[1;32m   3092\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3093\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/pegasus/modeling_pegasus.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1357\u001b[0m                 )\n\u001b[1;32m   1358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1359\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m   1360\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/pegasus/modeling_pegasus.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1220\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1221\u001b[0;31m         decoder_outputs = self.decoder(\n\u001b[0m\u001b[1;32m   1222\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_input_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/pegasus/modeling_pegasus.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1051\u001b[0m                 )\n\u001b[1;32m   1052\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1053\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m   1054\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/pegasus/modeling_pegasus.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0;31m# cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0mcross_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m             hidden_states, cross_attn_weights, cross_attn_present_key_value = self.encoder_attn(\n\u001b[0m\u001b[1;32m    434\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m                 \u001b[0mkey_value_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/pegasus/modeling_pegasus.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_cross_attention\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0;31m# cross_attentions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m             \u001b[0mkey_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_value_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbsz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m             \u001b[0mvalue_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_value_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbsz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/pegasus/modeling_pegasus.py\u001b[0m in \u001b[0;36m_shape\u001b[0;34m(self, tensor, seq_len, bsz)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbsz\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     def forward(\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 26.81 MiB is free. Process 720774 has 14.72 GiB memory in use. Of the allocated memory 13.89 GiB is allocated by PyTorch, and 17.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "rouge_metric = load_metric('rouge')\n",
        "\n",
        "score = calculate_metric_on_test_ds(dataset_samsum['test'], rouge_metric, model_pegasus, tokenizer, column_text = 'dialogue', column_summary='summary', batch_size=8)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "VOpMXVoF3lsH",
        "outputId": "da142492-0f27-4c4f-c670-946d172034cb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-06180949-b67a-42d5-ab30-a95a863bd042\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rouge1</th>\n",
              "      <th>rouge2</th>\n",
              "      <th>rougeL</th>\n",
              "      <th>rougeLsum</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>pegasus</th>\n",
              "      <td>0.015553</td>\n",
              "      <td>0.000302</td>\n",
              "      <td>0.015557</td>\n",
              "      <td>0.015492</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-06180949-b67a-42d5-ab30-a95a863bd042')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-06180949-b67a-42d5-ab30-a95a863bd042 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-06180949-b67a-42d5-ab30-a95a863bd042');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "           rouge1    rouge2    rougeL  rougeLsum\n",
              "pegasus  0.015553  0.000302  0.015557   0.015492"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
        "rouge_dict = dict((rn, score[rn].mid.fmeasure ) for rn in rouge_names )\n",
        "\n",
        "pd.DataFrame(rouge_dict, index = ['pegasus'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "rLa0sg__v_di",
        "outputId": "3b2e3612-bb8b-46fc-b4b6-dc0b00a8c3d7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1044 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA90AAAGGCAYAAABmGOKbAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABEUUlEQVR4nO3dfVhUdf7/8dcAAiqClgGKLCCa93fr3Y/MNCOpzJba0jUXlbzJUjPJtdxU0krU0rCytSy1LV3NStfdFFOUSqU1TUtbtUxRM0UtBW9B4fP7oy+zjoDCwGG4eT6ua66az3zOmfc5Au95zZw5x2aMMQIAAAAAAKXOzdUFAAAAAABQWRG6AQAAAACwCKEbAAAAAACLELoBAAAAALAIoRsAAAAAAIsQugEAAAAAsAihGwAAAAAAixC6AQAAAACwCKEbAAAAAACLELqBQjz33HOy2WxOLdu9e3d17969dAuqwNLS0mSz2fTyyy+7upQKLSUlRTabTR9++KGrSwEAVGALFy6UzWbT1q1bXV1KhZb3WvHkyZOuLgXlHKEbVUJec8m7eXt7q379+oqKitKrr76qM2fOuLrEcicvKBfllpaW5upyiyU0NFT33nuvq8so1OLFi5WYmOjqMgCgxHbu3KkHH3xQISEh8vb2VlBQkO6880699tprri6twrn6tUxht9DQUFeXWiwV4Y35qVOnasWKFa4uAxWYh6sLAMrSlClTFBYWpkuXLunYsWNKSUnRk08+qVmzZmnlypVq3bq1fe6ECRP0zDPPuLBa17rpppv03nvvOYzNnDlTP/30k1555ZV8c1F6Fi9erF27dunJJ590dSkA4LTNmzfr9ttv1+9+9zsNHTpUgYGBOnz4sL788kvNnj1bo0aNcnWJFcptt92Wry8PGTJEnTp10rBhw+xjPj4+ZV1apTd16lQ9+OCDio6OdnUpqKAI3ahS7r77bnXo0MF+f/z48Vq/fr3uvfde3Xfffdq9e7eqV68uSfLw8JCHR9X9FalZs6b+/Oc/O4wtWbJEp06dyjcOAMDVXnzxRfn5+emrr75S7dq1HR47fvy4a4pyIWOMLl68aH+dUVwNGzZUw4YNHcaGDx+uhg0b0peBco7Dy1Hl9ejRQxMnTtTBgwf1/vvv28cL+k73ggUL1KNHD/n7+8vLy0vNmzfX3/72tyI9z/HjxzV48GAFBATI29tbbdq00bvvvptv3i+//KKYmBj5+vqqdu3aGjhwoL755hvZbDYtXLjQPq+w740PGjQo36Flubm5SkxMVIsWLeTt7a2AgAA9+uijOnXqVJFqL43tupoxRsOGDZOnp6c+/vhj+/j777+v9u3bq3r16rrhhhv0pz/9SYcPH3ZYtnv37mrZsqX++9//6vbbb1eNGjUUFBSkGTNmlHh7rlTatRw8eFD33XefatasKX9/f40ZM0Zr1qyRzWZTSkqKfX2ffPKJDh48WOihgrm5uXrxxRfVoEEDeXt764477tC+fftKddsBoKR+/PFHtWjRIl/gliR/f3/7/+cdXnxlj8tjs9n03HPP2e/n9ebvv/9ef/7zn+Xn56ebbrpJEydOlDFGhw8f1h/+8Af5+voqMDBQM2fOdFhf3rkxPvjgA02ePFlBQUGqVauWHnzwQWVkZCgrK0tPPvmk/P395ePjo9jYWGVlZTmso6ivBfK+yrRmzRp16NBB1atX15tvvqlu3bqpTZs2Be6zJk2aKCoq6hp79fq2b9+uu+++W76+vvLx8dEdd9yhL7/88rrLnTp1Sp06dVKDBg20d+9eSVJWVpbi4+PVqFEjeXl5KTg4WOPGjcu3T2w2m0aOHKkVK1aoZcuW8vLyUosWLZSUlFSibbmSFbWkpKSoQ4cO8vb2Vnh4uN588818r/9sNpvOnTund999196XBw0a5LCe06dPa9CgQapdu7b8/PwUGxur8+fPl9q2o+Kruh/jAVeIiYnRX//6V3366acaOnRoofP+9re/qUWLFrrvvvvk4eGhf/3rX3r88ceVm5urESNGFLrchQsX1L17d+3bt08jR45UWFiYli1bpkGDBun06dMaPXq0pN/CVO/evbVlyxY99thjatq0qf75z39q4MCBJdq+Rx99VAsXLlRsbKyeeOIJHThwQK+//rq2b9+uTZs2qVq1ak6tt6jbdbWcnBw98sgjWrp0qZYvX65evXpJ+u1TkYkTJ6pPnz4aMmSITpw4oddee0233Xabtm/f7vDC7dSpU7rrrrv0wAMPqE+fPvrwww/19NNPq1WrVrr77rud2p4rlXYt586dU48ePXT06FGNHj1agYGBWrx4sTZs2ODwvM8++6wyMjIcDuO/+lDBadOmyc3NTWPHjlVGRoZmzJih/v376z//+U+JtxsASktISIhSU1O1a9cutWzZslTX3bdvXzVr1kzTpk3TJ598ohdeeEE33HCD3nzzTfXo0UPTp0/XokWLNHbsWHXs2FG33Xabw/IJCQmqXr26nnnmGe3bt0+vvfaaqlWrJjc3N506dUrPPfecvvzySy1cuFBhYWGaNGmSfdnivBbYu3ev+vXrp0cffVRDhw5VkyZN5OPjo6FDh+bbL1999ZW+//57TZgwwen98t1336lr167y9fXVuHHjVK1aNb355pvq3r27PvvsM3Xu3LnA5U6ePKk777xTv/76qz777DOFh4crNzdX9913nzZu3Khhw4apWbNm2rlzp1555RV9//33+b7jvHHjRn388cd6/PHHVatWLb366qv64x//qEOHDunGG290epskWVLL9u3bddddd6levXqaPHmycnJyNGXKlHxfmXvvvffyHcYfHh7uMKdPnz4KCwtTQkKCvv76a7399tvy9/fX9OnTS7TdqEQMUAUsWLDASDJfffVVoXP8/PxMu3bt7Pfj4+PN1b8i58+fz7dcVFSUadiwocNYt27dTLdu3ez3ExMTjSTz/vvv28eys7NNRESE8fHxMZmZmcYYYz766CMjySQmJtrn5eTkmB49ehhJZsGCBYU+R56BAweakJAQ+/0vvvjCSDKLFi1ymJeUlFTg+LX06tXLYd1F3a4DBw4YSeall14yly5dMn379jXVq1c3a9assS+XlpZm3N3dzYsvvujwnDt37jQeHh4O4926dTOSzN///nf7WFZWlgkMDDR//OMfr7sdISEhplevXoU+bkUtM2fONJLMihUr7GMXLlwwTZs2NZLMhg0b7ONX7+c8GzZsMJJMs2bNTFZWln189uzZRpLZuXPndbcdAMrKp59+atzd3Y27u7uJiIgw48aNM2vWrDHZ2dkO8/J6xJU9Lo8kEx8fb7+f15uHDRtmH7t8+bJp0KCBsdlsZtq0afbxU6dOmerVq5uBAwfax/L+jrZs2dKhjn79+hmbzWbuvvtuh+ePiIjI9/e4qK8FQkJCjCSTlJTkMH769Gnj7e1tnn76aYfxJ554wtSsWdOcPXs23/oLU7NmTYfti46ONp6enubHH3+0j/3888+mVq1a5rbbbrOPXfm66OjRo6ZFixamYcOGJi0tzT7nvffeM25ubuaLL75weM65c+caSWbTpk32MUnG09PT7Nu3zz72zTffGEnmtddeu+Y2XPkaoTBW1NK7d29To0YNc+TIEfvYDz/8YDw8PPK9/rt6P+fJ+3l85JFHHMbvv/9+c+ONN15zu1G1cHg58H98fHyuexbzK7+HlZGRoZMnT6pbt27av3+/MjIyCl1u1apVCgwMVL9+/exj1apV0xNPPKGzZ8/qs88+kyQlJSWpWrVqDp+2u7m5XfNT9OtZtmyZ/Pz8dOedd+rkyZP2W/v27eXj45Pvk9biKOp25cnOztZDDz2kf//731q1apV69uxpf+zjjz9Wbm6u+vTp41BnYGCgGjdunK9OHx8fh++weXp6qlOnTtq/f7/T22NlLUlJSQoKCtJ9991nH/P29r7mkRWFiY2Nlaenp/1+165dJalUth0ASsudd96p1NRU3Xffffrmm280Y8YMRUVFKSgoSCtXrizRuocMGWL/f3d3d3Xo0EHGGA0ePNg+Xrt2bTVp0qTAv40DBgxwOMqrc+fOMsbokUcecZjXuXNnHT58WJcvX7aPFee1QFhYWL7Dxf38/PSHP/xB//jHP2SMkfTbEWBLly5VdHS0atasWZxdYZeTk6NPP/1U0dHRDt/9rlevnh5++GFt3LhRmZmZDsv89NNP6tatmy5duqTPP/9cISEh9seWLVumZs2aqWnTpg69sEePHpKUrxdGRkY6fALcunVr+fr6lkpvKu1acnJytG7dOkVHR6t+/fr2eY0aNXLqaLnhw4c73O/atat++eWXfPsbVReHlwP/5+zZsw7fMSvIpk2bFB8fr9TU1Hzf1cnIyJCfn1+Byx08eFCNGzeWm5vj+1zNmjWzP57333r16qlGjRoO8xo1alSsbbnSDz/8oIyMjEK3rSQnsynqduVJSEjQ2bNntXr16nzfR//hhx9kjFHjxo0LfK6rD4Fv0KBBvu/c16lTR99++60zm2J5LQcPHlR4eHi+ec782/7ud7/L91ySSuU7+gBQmjp27KiPP/5Y2dnZ+uabb7R8+XK98sorevDBB7Vjxw41b97cqfVe/XfQz89P3t7eqlu3br7xX375pUjLS1JwcHC+8dzcXGVkZNgPSy7Oa4GwsLAC6x8wYICWLl2qL774QrfddpvWrVun9PR0xcTEXGuzr+nEiRM6f/68mjRpku+xZs2aKTc3V4cPH1aLFi3s4zExMfLw8NDu3bsVGBjosMwPP/yg3bt3F3qFkqtfP1y9T6Xf+lNp9KbSruX48eO6cOFCgT24tPuyr69vsdeHyofQDei3d3ozMjKu+Yf2xx9/1B133KGmTZtq1qxZCg4Olqenp1atWqVXXnlFubm5ZVjxbyf2yHuH/Eo5OTkO93Nzc+Xv769FixYVuJ6yvNxXVFSUkpKSNGPGDHXv3l3e3t72x3Jzc2Wz2bR69Wq5u7vnW/bq7zUXNEdSgfukuMpTLQUp6+cDgJLy9PRUx44d1bFjR918882KjY3VsmXLFB8fn+/NyDxX97MrFfR3sDh/Gwube711FPe1QGFnKo+KilJAQIDef/993XbbbXr//fcVGBioyMjIAudb5YEHHtDf//53zZ49WwkJCQ6P5ebmqlWrVpo1a1aBy179BoXVfbm81FIQ+jKuh9ANSPbrXl7rjKH/+te/lJWVpZUrVzq8o1mUw7NDQkL07bffKjc31+FT4T179tgfz/vvhg0bdP78eYdPuws6M3WdOnUKPGTr6k+Xw8PDtW7dOnXp0sXpy5QUpqjblef//b//p+HDh+vee+/VQw89pOXLl9svyxYeHi5jjMLCwnTzzTeXap3FZUUtISEh+u9//ytjjMMLzIL+bQt7AQoAlUHepTuPHj0q6X+fCp4+fdph3tX9rDwoyWuBK7m7u+vhhx/WwoULNX36dK1YsUJDhw4tNLwVxU033aQaNWrYzzx+pT179sjNzS1fOB01apQaNWqkSZMmyc/PT88884z9sfDwcH3zzTe64447XN6XSrsWf39/eXt7F9iD6cuwAt/pRpW3fv16Pf/88woLC1P//v0LnZfXCK981zIjI0MLFiy47nPcc889OnbsmJYuXWofu3z5sl577TX5+PioW7dukn4L/ZcuXdK8efPs83JzczVnzpx86wwPD9eePXt04sQJ+9g333yjTZs2Oczr06ePcnJy9Pzzz+dbx+XLl/O9yCmOom7XlSIjI7VkyRIlJSUpJibG/qnAAw88IHd3d02ePDnfO8PGmAIPD7SKFbVERUXpyJEjDt9jvHjxosO/dZ6aNWte8xwBAFARbNiwocBP+latWiVJ9sOgfX19VbduXX3++ecO89544w3riyymkrwWuFpMTIxOnTqlRx99VGfPni3xtbbd3d3Vs2dP/fOf/1RaWpp9PD09XYsXL9att95a4KHOEydO1NixYzV+/HiHS5/16dNHR44cKbBPXbhwQefOnStRvcVR2rW4u7srMjJSK1as0M8//2wf37dvn1avXp1vfs2aNUv0egngk25UKatXr9aePXt0+fJlpaena/369Vq7dq1CQkK0cuVKh8Odr9azZ095enqqd+/e9gY5b948+fv729+tL8ywYcP05ptvatCgQdq2bZtCQ0P14YcfatOmTUpMTFStWrUkSdHR0erUqZOeeuop7du3T02bNtXKlSv166+/SnJ8p/WRRx7RrFmzFBUVpcGDB+v48eOaO3euWrRo4XDijm7duunRRx9VQkKCduzYoZ49e6patWr64YcftGzZMs2ePVsPPvigU/uzqNt1tejoaC1YsEADBgyQr6+v3nzzTYWHh+uFF17Q+PHjlZaWpujoaNWqVUsHDhzQ8uXLNWzYMI0dO9apOguyb98+vfDCC/nG27Vrp169epV6LY8++qhef/119evXT6NHj1a9evW0aNEi+8/clf+27du319KlSxUXF6eOHTvKx8dHvXv3LtkGA0AZGzVqlM6fP6/7779fTZs2VXZ2tjZv3qylS5cqNDRUsbGx9rlDhgzRtGnTNGTIEHXo0EGff/65vv/+exdWX7CSvBa4Wrt27dSyZUv7ScJ+//vfl7i+F154QWvXrtWtt96qxx9/XB4eHnrzzTeVlZWlGTNmFLrcSy+9pIyMDI0YMUK1atXSn//8Z8XExOiDDz7Q8OHDtWHDBnXp0kU5OTnas2ePPvjgA/v1x0tLcnKyLl68mG88Ojraklqee+45ffrpp+rSpYsee+wx5eTk6PXXX1fLli21Y8cOh7nt27fXunXrNGvWLNWvX19hYWGFXn4NKFDZnSgdcJ28S2Pk3Tw9PU1gYKC58847zezZs+2XtrpSQZcMW7lypWndurXx9vY2oaGhZvr06Wb+/PlGkjlw4IB9XkGX80pPTzexsbGmbt26xtPT07Rq1arAy6OcOHHCPPzww6ZWrVrGz8/PDBo0yGzatMlIMkuWLHGY+/7775uGDRsaT09P07ZtW7NmzZp8lwzL89Zbb5n27dub6tWrm1q1aplWrVqZcePGmZ9//rnI+7GgS1kVZbsKuxzIG2+8YSSZsWPH2sc++ugjc+utt5qaNWuamjVrmqZNm5oRI0aYvXv32ud069bNtGjRIl99hW371fIu41LQbfDgwZbVsn//ftOrVy9TvXp1c9NNN5mnnnrKfpm4L7/80j7v7Nmz5uGHHza1a9c2kuzrybvUzbJlyxzWe63L7QCAq6xevdo88sgjpmnTpsbHx8d4enqaRo0amVGjRpn09HSHuefPnzeDBw82fn5+platWqZPnz7m+PHjhV4y7MSJEw7LDxw40NSsWTNfDVf/jS7s72hhlxYt6PmK+lrgepenNMaYGTNmGElm6tSp15xXmIIuZfX111+bqKgo4+PjY2rUqGFuv/12s3nz5utub05OjunXr5/x8PCwX94yOzvbTJ8+3bRo0cJ4eXmZOnXqmPbt25vJkyebjIwM+7KSzIgRI/LVFxISUuCltq6U18MKu7333nuW1ZKcnGzatWtnPD09TXh4uHn77bfNU089Zby9vR3m7dmzx9x2222mevXqRpJ9PYX9PObt3yt/HlC12YzhG/5AebdixQrdf//92rhxo7p06eLqclCKEhMTNWbMGP30008KCgpydTkAgDI0e/ZsjRkzRmlpaQWecRtlLzo6Wt99951++OEHV5eCSoTQDZQzFy5ccDjhWU5Ojnr27KmtW7fq2LFjpX4yNJSdq/9tL168qHbt2iknJ6dcHkYJALCOMUZt2rTRjTfeWOwTsaF0XN2Xf/jhB7Vo0UIDBw4s8PvjgLP4TjdQzowaNUoXLlxQRESEsrKy9PHHH2vz5s2aOnUqgbuCe+CBB/S73/1Obdu2VUZGht5//33t2bOn0Mu5AQAqn3PnzmnlypXasGGDdu7cqX/+85+uLqnKatiwoQYNGqSGDRvq4MGD+tvf/iZPT0+NGzfO1aWhkuGTbqCcWbx4sWbOnKl9+/bp4sWLatSokR577DGNHDnS1aWhhBITE/X2228rLS1NOTk5at68ucaNG6e+ffu6ujQAQBlJS0tTWFiYateurccff1wvvviiq0uqsmJjY7VhwwYdO3ZMXl5eioiI0NSpU0vlpHbAlQjdAAAAAABYhOt0AwAAAABgEUI3AAAAAAAWqXInUsvNzdXPP/+sWrVqyWazubocAACcYozRmTNnVL9+fbm5Vaz30OnFAIDKoKi9uMqF7p9//lnBwcGuLgMAgFJx+PBhNWjQwNVlFAu9GABQmVyvF1e50F2rVi1Jv+0YX19fF1cDAIBzMjMzFRwcbO9rFQm9GABQGRS1F1e50J13GJuvry+NHgBQ4VXEw7PpxQCAyuR6vbhifQkMAAAAAIAKhNANAAAAAIBFCN0AAAAAAFiE0A0AAAAAgEUI3QAAAAAAWITQDQAAAACARQjdAAAAAABYxKWh+/PPP1fv3r1Vv3592Ww2rVix4rrLpKSk6Pe//728vLzUqFEjLVy40PI6AQCorOjFAABYy6Wh+9y5c2rTpo3mzJlTpPkHDhxQr169dPvtt2vHjh168sknNWTIEK1Zs8biSgEAqJzoxQAAWMvDlU9+99136+677y7y/Llz5yosLEwzZ86UJDVr1kwbN27UK6+8oqioKKvKBACg0qIXAwBgrQr1ne7U1FRFRkY6jEVFRSk1NdVFFQEAULXQiwEAKB6XftJdXMeOHVNAQIDDWEBAgDIzM3XhwgVVr1493zJZWVnKysqy38/MzLS8TgDSkdMXdOpcdqmtr05NTwXVzv87DqBs0YuBioNeDJQPFSp0OyMhIUGTJ092dRlAlXLk9AX1eDlFWZdzS22dXh5uWj+2O80eqIDoxUDZoxcD5UeFOrw8MDBQ6enpDmPp6eny9fUt8J11SRo/frwyMjLst8OHD5dFqUCVdupcdqk2eUnKupxbqu/WA3AOvRioGOjFQPlRoT7pjoiI0KpVqxzG1q5dq4iIiEKX8fLykpeXl9WlAQBQJdCLAQAoHpd+0n327Fnt2LFDO3bskPTbZUh27NihQ4cOSfrtnfEBAwbY5w8fPlz79+/XuHHjtGfPHr3xxhv64IMPNGbMGFeUDwBAhUcvBgDAWi4N3Vu3blW7du3Url07SVJcXJzatWunSZMmSZKOHj1qb/qSFBYWpk8++URr165VmzZtNHPmTL399ttcogQAACfRiwEAsJZLDy/v3r27jDGFPr5w4cICl9m+fbuFVQEAUHXQiwEAsFaFOpEaAAAAAAAVCaEbAAAAAACLELoBAAAAALAIoRsAAAAAAIsQugEAAAAAsAihGwAAAAAAixC6AQAAAACwCKEbAAAAAACLELoBAAAAALAIoRsAAAAAAIsQugEAAAAAsAihGwAAAAAAixC6AQAAAACwCKEbAAAAAACLELoBAAAAALAIoRsAAAAAAIsQugEAAAAAsAihGwAAAAAAixC6AQAAAACwCKEbAAAAAACLELoBAAAAALAIoRsAAAAAAIsQugEAAAAAsAihGwAAAAAAixC6AQAAAACwCKEbAAAAAACLELoBAAAAALAIoRsAAAAAAIsQugEAAAAAsAihGwAAAAAAixC6AQAAAACwCKEbAAAAAACLELoBAAAAALAIoRsAAAAAAIsQugEAAAAAsAihGwAAAAAAixC6AQAAAACwCKEbAAAAAACLELoBAAAAALAIoRsAAAAAAIsQugEAAAAAsAihGwAAAAAAixC6AQAAAACwCKEbAAAAAACLELoBAAAAALAIoRsAAAAAAIsQugEAAAAAsIjLQ/ecOXMUGhoqb29vde7cWVu2bLnm/MTERDVp0kTVq1dXcHCwxowZo4sXL5ZRtQAAVD70YgAArOPS0L106VLFxcUpPj5eX3/9tdq0aaOoqCgdP368wPmLFy/WM888o/j4eO3evVvvvPOOli5dqr/+9a9lXDkAAJUDvRgAAGu5NHTPmjVLQ4cOVWxsrJo3b665c+eqRo0amj9/foHzN2/erC5duujhhx9WaGioevbsqX79+l33HXkAAFAwejEAANZyWejOzs7Wtm3bFBkZ+b9i3NwUGRmp1NTUApe55ZZbtG3bNntj379/v1atWqV77rmnTGoGAKAyoRcDAGA9D1c98cmTJ5WTk6OAgACH8YCAAO3Zs6fAZR5++GGdPHlSt956q4wxunz5soYPH37NQ9qysrKUlZVlv5+ZmVk6GwAAQAVHLwYAwHouP5FacaSkpGjq1Kl644039PXXX+vjjz/WJ598oueff77QZRISEuTn52e/BQcHl2HFAABULvRiAACKx2WfdNetW1fu7u5KT093GE9PT1dgYGCBy0ycOFExMTEaMmSIJKlVq1Y6d+6chg0bpmeffVZubvnfQxg/frzi4uLs9zMzM2n2AACIXgwAQFlw2Sfdnp6eat++vZKTk+1jubm5Sk5OVkRERIHLnD9/Pl8zd3d3lyQZYwpcxsvLS76+vg43AABALwYAoCy47JNuSYqLi9PAgQPVoUMHderUSYmJiTp37pxiY2MlSQMGDFBQUJASEhIkSb1799asWbPUrl07de7cWfv27dPEiRPVu3dve8MHAABFRy8GAMBaLg3dffv21YkTJzRp0iQdO3ZMbdu2VVJSkv2ELocOHXJ4N33ChAmy2WyaMGGCjhw5optuukm9e/fWiy++6KpNAACgQqMXAwBgLZsp7FiwSiozM1N+fn7KyMjg8DbAIruOZOje1zaW+nr/PepWtQzyK/X1AhVRRe5nFbl2oKKgFwPWK2o/q1BnLwcAAAAAoCIhdAMAAAAAYBFCNwAAAAAAFiF0AwAAAABgEUI3AAAAAAAWIXQDAAAAAGARQjcAAAAAABYhdAMAAAAAYBFCNwAAAAAAFiF0AwAAAABgEUI3AAAAAAAWIXQDAAAAAGARQjcAAAAAABYhdAMAAAAAYBFCNwAAAAAAFiF0AwAAAABgEUI3AAAAAAAWIXQDAAAAAGARQjcAAAAAABYhdAMAAAAAYBFCNwAAAAAAFiF0AwAAAABgEUI3AAAAAAAWIXQDAAAAAGARQjcAAAAAABYhdAMAAAAAYBFCNwAAAAAAFiF0AwAAAABgEUI3AAAAAAAWIXQDAAAAAGARQjcAAAAAABYhdAMAAAAAYBFCNwAAAAAAFiF0AwAAAABgEUI3AAAAAAAWIXQDAAAAAGARQjcAAAAAABYhdAMAAAAAYBFCNwAAAAAAFiF0AwAAAABgEUI3AAAAAAAWIXQDAAAAAGARQjcAAAAAABYhdAMAAAAAYBFCNwAAAAAAFiF0AwAAAABgEUI3AAAAAAAWcXnonjNnjkJDQ+Xt7a3OnTtry5Yt15x/+vRpjRgxQvXq1ZOXl5duvvlmrVq1qoyqBQCg8qEXAwBgHQ9XPvnSpUsVFxenuXPnqnPnzkpMTFRUVJT27t0rf3//fPOzs7N15513yt/fXx9++KGCgoJ08OBB1a5du+yLBwCgEqAXAwBgLZeG7lmzZmno0KGKjY2VJM2dO1effPKJ5s+fr2eeeSbf/Pnz5+vXX3/V5s2bVa1aNUlSaGhoWZYMAEClQi8GAMBaLju8PDs7W9u2bVNkZOT/inFzU2RkpFJTUwtcZuXKlYqIiNCIESMUEBCgli1baurUqcrJySmrsgEAqDToxQAAWM9ln3SfPHlSOTk5CggIcBgPCAjQnj17Clxm//79Wr9+vfr3769Vq1Zp3759evzxx3Xp0iXFx8cXuExWVpaysrLs9zMzM0tvIwAAqMDoxQAAWM/lJ1IrjtzcXPn7++utt95S+/bt1bdvXz377LOaO3duocskJCTIz8/PfgsODi7DigEAqFzoxQAAFI/LQnfdunXl7u6u9PR0h/H09HQFBgYWuEy9evV08803y93d3T7WrFkzHTt2TNnZ2QUuM378eGVkZNhvhw8fLr2NAACgAqMXAwBgPZeFbk9PT7Vv317Jycn2sdzcXCUnJysiIqLAZbp06aJ9+/YpNzfXPvb999+rXr168vT0LHAZLy8v+fr6OtwAAAC9GACAsuBU6G7YsKF++eWXfOOnT59Ww4YNi7yeuLg4zZs3T++++652796txx57TOfOnbOfQXXAgAEaP368ff5jjz2mX3/9VaNHj9b333+vTz75RFOnTtWIESOc2QwAAKo8ejEAANZy6kRqaWlpBZ6lNCsrS0eOHCnyevr27asTJ05o0qRJOnbsmNq2baukpCT7CV0OHTokN7f/vS8QHBysNWvWaMyYMWrdurWCgoI0evRoPf30085sBgAAVR69GAAAaxUrdK9cudL+/2vWrJGfn5/9fk5OjpKTk4t9rc6RI0dq5MiRBT6WkpKSbywiIkJffvllsZ4DAAAUjl4MAIB1ihW6o6OjJUk2m00DBw50eKxatWoKDQ3VzJkzS604AAAAAAAqsmKF7ryTpoSFhemrr75S3bp1LSkKAAAAAIDKwKnvdB84cKC06wAAAAAAoNJxKnRLUnJyspKTk3X8+HGHy4ZI0vz580tcGAAAAAAAFZ1ToXvy5MmaMmWKOnTooHr16slms5V2XQAAAAAAVHhOhe65c+dq4cKFiomJKe16AAAAAACoNNyuPyW/7Oxs3XLLLaVdCwAAAAAAlYpToXvIkCFavHhxadcCAAAAAECl4tTh5RcvXtRbb72ldevWqXXr1qpWrZrD47NmzSqV4gAAAAAAqMicCt3ffvut2rZtK0natWuXw2OcVA0AAAAAgN84Fbo3bNhQ2nUAAAAAAFDpOPWdbgAAAAAAcH1OfdJ9++23X/Mw8vXr1ztdEAAAAAAAlYVToTvv+9x5Ll26pB07dmjXrl0aOHBgadQFAAAAAECF51TofuWVVwocf+6553T27NkSFQQAAAAAQGVRqt/p/vOf/6z58+eX5ioBAAAAAKiwSjV0p6amytvbuzRXCQAAAABAheXU4eUPPPCAw31jjI4ePaqtW7dq4sSJpVIYAAAAAAAVnVOh28/Pz+G+m5ubmjRpoilTpqhnz56lUhgAAAAAABWdU6F7wYIFpV0HAAAAAACVjlOhO8+2bdu0e/duSVKLFi3Url27UikKAAAAAIDKwKnQffz4cf3pT39SSkqKateuLUk6ffq0br/9di1ZskQ33XRTadYIAAAAAECF5NTZy0eNGqUzZ87ou+++06+//qpff/1Vu3btUmZmpp544onSrhEAAAAAgArJqU+6k5KStG7dOjVr1sw+1rx5c82ZM4cTqQEAAAAA8H+c+qQ7NzdX1apVyzderVo15ebmlrgoAAAAAAAqA6dCd48ePTR69Gj9/PPP9rEjR45ozJgxuuOOO0qtOAAAAAAAKjKnQvfrr7+uzMxMhYaGKjw8XOHh4QoLC1NmZqZee+210q4RAAAAAIAKyanvdAcHB+vrr7/WunXrtGfPHklSs2bNFBkZWarFAQAAAABQkRXrk+7169erefPmyszMlM1m05133qlRo0Zp1KhR6tixo1q0aKEvvvjCqloBAAAAAKhQihW6ExMTNXToUPn6+uZ7zM/PT48++qhmzZpVasUBAAAAAFCRFSt0f/PNN7rrrrsKfbxnz57atm1biYsCAAAAAKAyKFboTk9PL/BSYXk8PDx04sSJEhcFAAAAAEBlUKzQHRQUpF27dhX6+Lfffqt69eqVuCgAAAAAACqDYoXue+65RxMnTtTFixfzPXbhwgXFx8fr3nvvLbXiAAAAAACoyIp1ybAJEybo448/1s0336yRI0eqSZMmkqQ9e/Zozpw5ysnJ0bPPPmtJoQAAAAAAVDTFCt0BAQHavHmzHnvsMY0fP17GGEmSzWZTVFSU5syZo4CAAEsKBQAAAACgoilW6JakkJAQrVq1SqdOndK+fftkjFHjxo1Vp04dK+oDAAAAAKDCKnbozlOnTh117NixNGsBAAAAAKBSKdaJ1AAAAAAAQNERugEAAAAAsAihGwAAAAAAixC6AQAAAACwCKEbAAAAAACLELoBAAAAALAIoRsAAAAAAIsQugEAAAAAsAihGwAAAAAAi5SL0D1nzhyFhobK29tbnTt31pYtW4q03JIlS2Sz2RQdHW1tgQAAVGL0YQAArOPy0L106VLFxcUpPj5eX3/9tdq0aaOoqCgdP378msulpaVp7Nix6tq1axlVCgBA5UMfBgDAWi4P3bNmzdLQoUMVGxur5s2ba+7cuapRo4bmz59f6DI5OTnq37+/Jk+erIYNG5ZhtQAAVC70YQAArOXS0J2dna1t27YpMjLSPubm5qbIyEilpqYWutyUKVPk7++vwYMHl0WZAABUSvRhAACs5+HKJz958qRycnIUEBDgMB4QEKA9e/YUuMzGjRv1zjvvaMeOHUV6jqysLGVlZdnvZ2ZmOl0vAACVSVn0YYleDACo2lx+eHlxnDlzRjExMZo3b57q1q1bpGUSEhLk5+dnvwUHB1tcJQAAlZMzfViiFwMAqjaXftJdt25dubu7Kz093WE8PT1dgYGB+eb/+OOPSktLU+/eve1jubm5kiQPDw/t3btX4eHhDsuMHz9ecXFx9vuZmZk0ewAAVDZ9WKIXAwCqNpeGbk9PT7Vv317Jycn2y43k5uYqOTlZI0eOzDe/adOm2rlzp8PYhAkTdObMGc2ePbvABu7l5SUvLy9L6gcAoCIriz4s0YsBAFWbS0O3JMXFxWngwIHq0KGDOnXqpMTERJ07d06xsbGSpAEDBigoKEgJCQny9vZWy5YtHZavXbu2JOUbBwAA10cfBgDAWi4P3X379tWJEyc0adIkHTt2TG3btlVSUpL9pC6HDh2Sm1uF+uo5AAAVBn0YAABr2YwxxtVFlKXMzEz5+fkpIyNDvr6+ri4HqJR2HcnQva9tLPX1/nvUrWoZ5Ffq6wUqoorczypy7UBFQS8GrFfUfsZb1wAAAAAAWITQDQAAAACARQjdAAAAAABYhNANAAAAAIBFCN0AAAAAAFiE0A0AAAAAgEUI3QAAAAAAWITQDQAAAACARQjdAAAAAABYhNANAAAAAIBFCN0AAAAAAFiE0A0AAAAAgEUI3QAAAAAAWITQDQAAAACARQjdAAAAAABYhNANAAAAAIBFCN0AAAAAAFiE0A0AAAAAgEUI3QAAAAAAWITQDQAAAACARQjdAAAAAABYhNANAAAAAIBFCN0AAAAAAFiE0A0AAAAAgEUI3QAAAAAAWITQDQAAAACARQjdAAAAAABYhNANAAAAAIBFCN0AAAAAAFiE0A0AAAAAgEUI3QAAAAAAWITQDQAAAACARQjdAAAAAABYhNANAAAAAIBFCN0AAAAAAFiE0A0AAAAAgEUI3QAAAAAAWITQDQAAAACARQjdAAAAAABYhNANAAAAAIBFCN0AAAAAAFiE0A0AAAAAgEUI3QAAAAAAWITQDQAAAACARQjdAAAAAABYhNANAAAAAIBFCN0AAAAAAFikXITuOXPmKDQ0VN7e3urcubO2bNlS6Nx58+apa9euqlOnjurUqaPIyMhrzgcAANdGHwYAwDouD91Lly5VXFyc4uPj9fXXX6tNmzaKiorS8ePHC5yfkpKifv36acOGDUpNTVVwcLB69uypI0eOlHHlAABUfPRhAACs5fLQPWvWLA0dOlSxsbFq3ry55s6dqxo1amj+/PkFzl+0aJEef/xxtW3bVk2bNtXbb7+t3NxcJScnl3HlAABUfPRhAACs5dLQnZ2drW3btikyMtI+5ubmpsjISKWmphZpHefPn9elS5d0ww03WFUmAACVEn0YAADrebjyyU+ePKmcnBwFBAQ4jAcEBGjPnj1FWsfTTz+t+vXrO7xguFJWVpaysrLs9zMzM50vGACASqQs+rBELwYAVG0uP7y8JKZNm6YlS5Zo+fLl8vb2LnBOQkKC/Pz87Lfg4OAyrhIAgMqpKH1YohcDAKo2l4buunXryt3dXenp6Q7j6enpCgwMvOayL7/8sqZNm6ZPP/1UrVu3LnTe+PHjlZGRYb8dPny4VGoHAKCiK4s+LNGLAQBVm0tDt6enp9q3b+9w8pW8k7FEREQUutyMGTP0/PPPKykpSR06dLjmc3h5ecnX19fhBgAAyqYPS/RiAEDV5tLvdEtSXFycBg4cqA4dOqhTp05KTEzUuXPnFBsbK0kaMGCAgoKClJCQIEmaPn26Jk2apMWLFys0NFTHjh2TJPn4+MjHx8dl2wEAQEVEHwYAwFouD919+/bViRMnNGnSJB07dkxt27ZVUlKS/aQuhw4dkpvb/z6Q/9vf/qbs7Gw9+OCDDuuJj4/Xc889V5alAwBQ4dGHAQCwlstDtySNHDlSI0eOLPCxlJQUh/tpaWnWFwQAQBVCHwYAwDoV+uzlAAAAAACUZ4RuAAAAAAAsQugGAAAAAMAihG4AAAAAACxC6AYAAAAAwCKEbgAAAAAALELoBgAAAADAIoRuAAAAAAAsQugGAAAAAMAihG4AAAAAACxC6AYAAAAAwCKEbgAAAAAALELoBgAAAADAIoRuAAAAAAAsQugGAAAAAMAihG4AAAAAACxC6AYAAAAAwCKEbgAAAAAALELoBgAAAADAIoRuAAAAAAAsQugGAAAAAMAihG4AAAAAACxC6AYAAAAAwCKEbgAAAAAALELoBgAAAADAIoRuAAAAAAAsQugGAAAAAMAihG4AAAAAACxC6AYAAAAAwCKEbgAAAAAALELoBgAAAADAIoRuAAAAAAAsQugGAAAAAMAihG4AAAAAACxC6AYAAAAAwCKEbgAAAAAALELoBgAAAADAIoRuAAAAAAAsQugGAAAAAMAihG4AAAAAACxC6AYAAAAAwCKEbgAAAAAALELoBgAAAADAIoRuAAAAAAAsQugGAAAAAMAihG4AAAAAACxC6AYAAAAAwCLlInTPmTNHoaGh8vb2VufOnbVly5Zrzl+2bJmaNm0qb29vtWrVSqtWrSqjSgEAqHzowwAAWMfloXvp0qWKi4tTfHy8vv76a7Vp00ZRUVE6fvx4gfM3b96sfv36afDgwdq+fbuio6MVHR2tXbt2lXHlAABUfPRhAACs5fLQPWvWLA0dOlSxsbFq3ry55s6dqxo1amj+/PkFzp89e7buuusu/eUvf1GzZs30/PPP6/e//71ef/31Mq4cAICKjz4MAIC1XBq6s7OztW3bNkVGRtrH3NzcFBkZqdTU1AKXSU1NdZgvSVFRUYXOBwAABaMPAwBgPQ9XPvnJkyeVk5OjgIAAh/GAgADt2bOnwGWOHTtW4Pxjx44VOD8rK0tZWVn2+xkZGZKkzMzMkpTu4ETmRZ04m3X9iUXkZpNyTamtrlyvrzzXVtXWV5rr2n/inHKzzpfOyq7w7f6jOnumdH53y/O/RWmvrzzXVtXWd5OPl27y9S6VdeX1MWOcL64s+rBELy7PP5Osr/ysq7TXV9V6cXn+t2B95Wddkmt6sUtDd1lISEjQ5MmT840HBwe7oBoAJdE/0dUVAOXPmTNn5Ofn5+oyroleDFQe9GIgv+v1YpeG7rp168rd3V3p6ekO4+np6QoMDCxwmcDAwGLNHz9+vOLi4uz3c3Nz9euvv+rGG2+UzWYr4RaUf5mZmQoODtbhw4fl6+vr6nIqFPad89h3JcP+c15V2nfGGJ05c0b169d3eh1l0Yelqt2Lq9LPpBXYf85j3zmPfVcyVWn/FbUXuzR0e3p6qn379kpOTlZ0dLSk3xpxcnKyRo4cWeAyERERSk5O1pNPPmkfW7t2rSIiIgqc7+XlJS8vL4ex2rVrl0b5FYqvr2+l/6G3CvvOeey7kmH/Oa+q7LuSfsJdFn1YohdLVedn0irsP+ex75zHviuZqrL/itKLXX54eVxcnAYOHKgOHTqoU6dOSkxM1Llz5xQbGytJGjBggIKCgpSQkCBJGj16tLp166aZM2eqV69eWrJkibZu3aq33nrLlZsBAECFRB8GAMBaLg/dffv21YkTJzRp0iQdO3ZMbdu2VVJSkv0kLYcOHZKb2/9Osn7LLbdo8eLFmjBhgv7617+qcePGWrFihVq2bOmqTQAAoMKiDwMAYC2Xh25JGjlyZKGHsaWkpOQbe+ihh/TQQw9ZXFXl4OXlpfj4+HyH9eH62HfOY9+VDPvPeew759CHrcPPZMmw/5zHvnMe+65k2H/52UxJrjUCAAAAAAAK5Xb9KQAAAAAAwBmEbgAAAAAALELoBgAAAADAIoTuCuzzzz9X7969Vb9+fdlsNq1YseK6y2RlZenZZ59VSEiIvLy8FBoaqvnz51tfbDnkzP5btGiR2rRpoxo1aqhevXp65JFH9Msvv1hfbDmSkJCgjh07qlatWvL391d0dLT27t173eWWLVumpk2bytvbW61atdKqVavKoNryx5n9N2/ePHXt2lV16tRRnTp1FBkZqS1btpRRxeWHsz97eZYsWSKbzWa/HjVQGujFzqMPO49e7Dz6cMnQi51D6K7Azp07pzZt2mjOnDlFXqZPnz5KTk7WO++8o7179+of//iHmjRpYmGV5Vdx99+mTZs0YMAADR48WN99952WLVumLVu2aOjQoRZXWr589tlnGjFihL788kutXbtWly5dUs+ePXXu3LlCl9m8ebP69eunwYMHa/v27YqOjlZ0dLR27dpVhpWXD87sv5SUFPXr108bNmxQamqqgoOD1bNnTx05cqQMK3c9Z/ZdnrS0NI0dO1Zdu3Ytg0pRldCLnUcfdh692Hn04ZKhFzvJoFKQZJYvX37NOatXrzZ+fn7ml19+KZuiKpCi7L+XXnrJNGzY0GHs1VdfNUFBQRZWVv4dP37cSDKfffZZoXP69OljevXq5TDWuXNn8+ijj1pdXrlXlP13tcuXL5tatWqZd99918LKyr+i7rvLly+bW265xbz99ttm4MCB5g9/+EPZFIgqh17sPPpwydCLnUcfLhl6cdHwSXcVsnLlSnXo0EEzZsxQUFCQbr75Zo0dO1YXLlxwdWkVQkREhA4fPqxVq1bJGKP09HR9+OGHuueee1xdmktlZGRIkm644YZC56SmpioyMtJhLCoqSqmpqZbWVhEUZf9d7fz587p06VKxlqmMirrvpkyZIn9/fw0ePLgsygKuiV7sPPpw4ejFzqMPlwy9uGg8XF0Ays7+/fu1ceNGeXt7a/ny5Tp58qQef/xx/fLLL1qwYIGryyv3unTpokWLFqlv3766ePGiLl++rN69exfrkMLKJjc3V08++aS6dOmili1bFjrv2LFjCggIcBgLCAjQsWPHrC6xXCvq/rva008/rfr16+d78VSVFHXfbdy4Ue+884527NhRdsUB10Avdh59uGD0YufRh0uGXlx0fNJdheTm5spms2nRokXq1KmT7rnnHs2aNUvvvvsu77AXwX//+1+NHj1akyZN0rZt25SUlKS0tDQNHz7c1aW5zIgRI7Rr1y4tWbLE1aVUSM7sv2nTpmnJkiVavny5vL29LayufCvKvjtz5oxiYmI0b9481a1btwyrAwpHL3Yefbhg9GLn0YdLhl5cdHzSXYXUq1dPQUFB8vPzs481a9ZMxhj99NNPaty4sQurK/8SEhLUpUsX/eUvf5EktW7dWjVr1lTXrl31wgsvqF69ei6usGyNHDlS//73v/X555+rQYMG15wbGBio9PR0h7H09HQFBgZaWWK5Vpz9l+fll1/WtGnTtG7dOrVu3driCsuvou67H3/8UWlpaerdu7d9LDc3V5Lk4eGhvXv3Kjw83PJ6gSvRi51HH86PXuw8+nDJ0IuLh0+6q5AuXbro559/1tmzZ+1j33//vdzc3Ir8x6YqO3/+vNzcHH9l3N3dJUnGGFeU5BLGGI0cOVLLly/X+vXrFRYWdt1lIiIilJyc7DC2du1aRUREWFVmueXM/pOkGTNm6Pnnn1dSUpI6dOhgcZXlU3H3XdOmTbVz507t2LHDfrvvvvt0++23a8eOHQoODi6jyoH/oRc7jz78P/Ri59GHS4Ze7CTXnL8NpeHMmTNm+/btZvv27UaSmTVrltm+fbs5ePCgMcaYZ555xsTExDjMb9CggXnwwQfNd999Zz777DPTuHFjM2TIEFdtgksVd/8tWLDAeHh4mDfeeMP8+OOPZuPGjaZDhw6mU6dOrtoEl3jssceMn5+fSUlJMUePHrXfzp8/b58TExNjnnnmGfv9TZs2GQ8PD/Pyyy+b3bt3m/j4eFOtWjWzc+dOV2yCSzmz/6ZNm2Y8PT3Nhx9+6LDMmTNnXLEJLuPMvrtaVTxjKqxFL3Yefdh59GLn0YdLhl7sHEJ3BbZhwwYjKd9t4MCBxpjffqC7devmsMzu3btNZGSkqV69umnQoIGJi4tz+CWpSpzZf6+++qpp3ry5qV69uqlXr57p37+/+emnn8q+eBcqaJ9JMgsWLLDP6datm30/5vnggw/MzTffbDw9PU2LFi3MJ598UraFlxPO7L+QkJACl4mPjy/z+l3J2Z+9K1XFRg9r0YudRx92Hr3YefThkqEXO8dmTBU7HgcAAAAAgDLCd7oBAAAAALAIoRsAAAAAAIsQugEAAAAAsAihGwAAAAAAixC6AQAAAACwCKEbAAAAAACLELoBAAAAALAIoRsAAAAAAIsQugGUS4MGDVJ0dLSry0AF9+KLL+qWW25RjRo1VLt27WIvP3z4cNlsNiUmJtrHUlJSZLPZCrx99dVXkqS9e/fq9ttvV0BAgLy9vdWwYUNNmDBBly5dcmo7srKy1LZtW9lsNu3YscOpdQBAcdCHUVroxYRuoMpzdVNNS0sjSKBEunfvroULFxb4WHZ2th566CE99thjxV7v8uXL9eWXX6p+/foO47fccouOHj3qcBsyZIjCwsLUoUMHSVK1atU0YMAAffrpp9q7d68SExM1b948xcfHF7sOSRo3bly+OgBUDvRhVAb04mvzcGopAAAqgMmTJ0tSoS8ECnPkyBGNGjVKa9asUa9evRwe8/T0VGBgoP3+pUuX9M9//lOjRo2SzWaTJDVs2FANGza0zwkJCVFKSoq++OILh3W9/fbbmjlzpg4cOKDQ0FA98cQTevzxxx3mrF69Wp9++qk++ugjrV69uljbAQCAq9GL+aQbwDXs2rVLd999t3x8fBQQEKCYmBidPHnS/nj37t31xBNPaNy4cbrhhhsUGBio5557zmEde/bs0a233ipvb281b95c69atk81m04oVKyRJYWFhkqR27drJZrOpe/fuDsu//PLLqlevnm688UaNGDHC6UOCgKLKzc1VTEyM/vKXv6hFixbXnb9y5Ur98ssvio2NLXTOvn37lJSUpG7dutnHFi1apEmTJunFF1/U7t27NXXqVE2cOFHvvvuufU56erqGDh2q9957TzVq1CjZhgGocOjDqKoqWy8mdAMo0OnTp9WjRw+1a9dOW7duVVJSktLT09WnTx+Hee+++65q1qyp//znP5oxY4amTJmitWvXSpJycnIUHR2tGjVq6D//+Y/eeustPfvssw7Lb9myRZK0bt06HT16VB9//LH9sQ0bNujHH3/Uhg0b9O6772rhwoXFfpcUKK7p06fLw8NDTzzxRJHmv/POO4qKilKDBg3yPXbLLbfI29tbjRs3VteuXTVlyhT7Y/Hx8Zo5c6YeeOABhYWF6YEHHtCYMWP05ptvSpKMMRo0aJCGDx9uP1QOQNVBH0ZVVul6sQFQpQ0cOND84Q9/yDf+/PPPm549ezqMHT582Egye/fuNcYY061bN3Prrbc6zOnYsaN5+umnjTHGrF692nh4eJijR4/aH1+7dq2RZJYvX26MMebAgQNGktm+fXu+ukJCQszly5ftYw899JDp27evs5uKSuLFF180NWvWtN/c3NyMl5eXw9jBgwcdllmwYIHx8/O77rq3bt1qAgICzJEjR+xjISEh5pVXXilw/uHDh42bm5v58MMPC3z80KFD5rvvvjOLFy82QUFBZvr06cYYY86ePWskmerVqzvU7eXlZfz9/Y0xxsyePdt06dLF/jtQ2O8KgIqNPoyKiF68/brbcSW+0w2gQN988402bNggHx+ffI/9+OOPuvnmmyVJrVu3dnisXr16On78uKTfzhoZHBzs8J2bTp06FbmGFi1ayN3d3WHdO3fuLNZ2oPIZPny4wyc9/fv31x//+Ec98MAD9jFnT3TyxRdf6Pjx4/rd735nH8vJydFTTz2lxMREpaWlOcxfsGCBbrzxRt13330Fri84OFiS1Lx5c+Xk5GjYsGF66qmndPbsWUnSvHnz1LlzZ4dl8n7m169fr9TUVHl5eTk83qFDB/Xv39/h0DcAlQ99GOUZvbh4vZjQDaBAZ8+eVe/evTV9+vR8j9WrV8/+/9WqVXN4zGazKTc3t1RqsHLdqLhuuOEG3XDDDfb71atXl7+/vxo1alTidcfExCgyMtJhLCoqSjExMfm+J2aM0YIFCzRgwIB8P6sFyc3N1aVLl5Sbm6uAgADVr19f+/fvV//+/Quc/+qrr+qFF16w3//5558VFRWlpUuX5ntxAKDyoQ+jPKMXF68XE7oBFOj3v/+9PvroI4WGhsrDw7k/FU2aNNHhw4eVnp6ugIAASbJfOzGPp6enpN/ewQRK26FDh/Trr7/q0KFDysnJsV8Sp1GjRvZPj5o2baqEhATdf//9uvHGG3XjjTc6rKNatWoKDAxUkyZNHMbXr1+vAwcOaMiQIfmed9GiRapWrZpatWolLy8vbd26VePHj1ffvn3tLwomT56sJ554Qn5+frrrrruUlZWlrVu36tSpU4qLi3N4h1+Svd7w8PACv7MGoHKhD6OyoBcTugFIysjIyHd9zmHDhmnevHnq16+f/ayo+/bt05IlS/T22287HG5WmDvvvFPh4eEaOHCgZsyYoTNnzmjChAmSZL+cg7+/v6pXr66kpCQ1aNBA3t7e8vPzK/VtRNU0adIkh0O/2rVrJ+m3kwPlnaF37969ysjIKPa633nnHd1yyy1q2rRpvsc8PDw0ffp0ff/99zLGKCQkRCNHjtSYMWPsc4YMGaIaNWropZde0l/+8hfVrFlTrVq10pNPPlnsWgBUbPRhVGb0YkI3AEkpKSn2P4B5Bg8erE2bNunpp59Wz549lZWVpZCQEN11111ycyvahQ/c3d21YsUKDRkyRB07dlTDhg310ksvqXfv3vL29pb02x/EV199VVOmTNGkSZPUtWtXpaSklPYmohK71s9LUc60a4y55uNXf3csz+LFiwtdpm/fvurbt+811ytJDz/8sB5++OHrzpOk0NDQ69YKoGKiD6Oioxdfm83QwQGUoU2bNunWW2/Vvn37FB4e7upyAACoUujDQNkjdAOw1PLly+Xj46PGjRtr3759Gj16tOrUqaONGze6ujQAACo9+jDgehxeDsBSZ86c0dNPP61Dhw6pbt26ioyM1MyZM11dFgAAVQJ9GHA9PukGAAAAAMAiRTsLAwAAAAAAKDZCNwAAAAAAFiF0AwAAAABgEUI3AAAAAAAWIXQDAAAAAGARQjcAAAAAABYhdAMAAAAAYBFCNwAAAAAAFiF0AwAAAABgkf8PzW7tWJITf+MAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 1000x400 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "dialogue_token_len = len([tokenizer.encode(s) for s in dataset_samsum['train']['dialogue']])\n",
        "\n",
        "summary_token_len = len([tokenizer.encode(s) for s in dataset_samsum['train']['summary']])\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
        "axes[0].hist(dialogue_token_len, bins = 20, color = 'C0', edgecolor = 'C0' )\n",
        "axes[0].set_title(\"Dialogue Token Length\")\n",
        "axes[0].set_xlabel(\"Length\")\n",
        "axes[0].set_ylabel(\"Count\")\n",
        "\n",
        "axes[1].hist(summary_token_len, bins = 20, color = 'C0', edgecolor = 'C0' )\n",
        "axes[1].set_title(\"Summary Token Length\")\n",
        "axes[1].set_xlabel(\"Length\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmDybCsu28BI"
      },
      "source": [
        "# **Training on custom data set**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "339580ead7ee4157b35050404a1409be",
            "78a9f8ec97b04bb2b7248dd4f38958c5",
            "d762ca4a6ffc442580dfafdff012c11e",
            "2d6170501b574013a935dc204eec2d8a",
            "68c9a8d24db24e6d9d992729e4118c71",
            "369272629e5049068464c417f2271cd4",
            "bcc690560e1744dba8288cc90be1984a",
            "0e422b86b5eb44e8b08a65f5f89330fc",
            "a155d65e3b364ad6a0589b1ac086ab46",
            "10bc78c1dcc64bd7898911448067bc2b",
            "af601fca2f6442deab5da73f1cdfae48"
          ]
        },
        "id": "i4p_SEOiwEUW",
        "outputId": "66627161-fe73-4eaf-b694-10aecaf39f68"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "339580ead7ee4157b35050404a1409be",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/819 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3856: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "def convert_examples_to_features(example_batch):\n",
        "    input_encodings = tokenizer(example_batch['dialogue'] , max_length = 1024, truncation = True )\n",
        "\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        target_encodings = tokenizer(example_batch['summary'], max_length = 128, truncation = True )\n",
        "\n",
        "    return {\n",
        "        'input_ids' : input_encodings['input_ids'],\n",
        "        'attention_mask': input_encodings['attention_mask'],\n",
        "        'labels': target_encodings['input_ids']\n",
        "    }\n",
        "\n",
        "dataset_samsum_pt = dataset_samsum.map(convert_examples_to_features, batched = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LTLdBah9wOAh"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model_pegasus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BGnlcLG0t3a"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "trainer_args = TrainingArguments(\n",
        "    output_dir='pegasus-samsum', num_train_epochs=1, warmup_steps=500,\n",
        "    per_device_train_batch_size=1, per_device_eval_batch_size=1,\n",
        "    weight_decay=0.01, logging_steps=10,\n",
        "    evaluation_strategy='steps', eval_steps=500, save_steps=1e6,\n",
        "    gradient_accumulation_steps=16\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p39XLQtT08yw"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(model=model_pegasus, args=trainer_args,\n",
        "                  tokenizer=tokenizer, data_collator=seq2seq_data_collator,\n",
        "                  train_dataset=dataset_samsum_pt[\"train\"],\n",
        "                  eval_dataset=dataset_samsum_pt[\"validation\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AP_iBWri3IKZ",
        "outputId": "8fc3521a-508a-41ea-aa5a-cd29d8ac7cb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/pegasus-samsum\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%cd /content/drive/MyDrive/pegasus-samsum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "rEWvo-Cw6VB8",
        "outputId": "30592adc-61d9-4219-99d3-95210e248f1c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You're using a PegasusTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='920' max='920' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [920/920 51:35, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.626500</td>\n",
              "      <td>1.484255</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=920, training_loss=1.8238315105438232, metrics={'train_runtime': 3104.1646, 'train_samples_per_second': 4.746, 'train_steps_per_second': 0.296, 'total_flos': 5526698901602304.0, 'train_loss': 1.8238315105438232, 'epoch': 1.0})"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1Vt5yO29H5Q"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fk4w6vsD9pnM",
        "outputId": "8e2cacc4-217a-4432-9c71-5fd22c4d7148"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('tokenizer/tokenizer_config.json',\n",
              " 'tokenizer/special_tokens_map.json',\n",
              " 'tokenizer/spiece.model',\n",
              " 'tokenizer/added_tokens.json',\n",
              " 'tokenizer/tokenizer.json')"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TW4QkChmKQsw",
        "outputId": "6c25d6f7-5694-4229-fe3d-28bebc58e35f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-34-90542a62301a>:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
            "  rouge_metric = load_metric('rouge')\n"
          ]
        }
      ],
      "source": [
        "rouge_metric = load_metric('rouge')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "3rPaMDaSKBuA",
        "outputId": "da34ea28-ed30-45bc-a75a-b6916d340847"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 410/410 [16:23<00:00,  2.40s/it]\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-fe7f46e90fba>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m )\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mrouge_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfmeasure\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrouge_names\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrouge_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34mf'pegasus'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'rouge_names' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "score = calculate_metric_on_test_ds(\n",
        "    dataset_samsum['test'], rouge_metric, trainer.model, tokenizer, batch_size = 2, column_text = 'dialogue', column_summary= 'summary'\n",
        ")\n",
        "\n",
        "rouge_dict = dict((rn, score[rn].mid.fmeasure ) for rn in rouge_names )\n",
        "\n",
        "pd.DataFrame(rouge_dict, index = [f'pegasus'] )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZmHaOTWyKCCk"
      },
      "outputs": [],
      "source": [
        "model_pegasus.save_pretrained(\"pegasus-samsum\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEE8blSiLyzv",
        "outputId": "94c4431f-25a1-4b0b-cccc-d6f7ab20d734"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('tokenizer/tokenizer_config.json',\n",
              " 'tokenizer/special_tokens_map.json',\n",
              " 'tokenizer/spiece.model',\n",
              " 'tokenizer/added_tokens.json',\n",
              " 'tokenizer/tokenizer.json')"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.save_pretrained(\"tokenizer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKqWxj44PB4B"
      },
      "source": [
        "# TEST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMyeRVNBPEY9"
      },
      "outputs": [],
      "source": [
        "\n",
        "dataset_samsum = load_dataset(\"samsum\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LzEh9FadPLE0"
      },
      "outputs": [],
      "source": [
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"tokenizer\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9cCwkbyPK8V"
      },
      "outputs": [],
      "source": [
        "\n",
        "sample_text = dataset_samsum[\"test\"][0][\"dialogue\"]\n",
        "\n",
        "reference = dataset_samsum[\"test\"][0][\"summary\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        },
        "id": "PE7WfNZPPR3k",
        "outputId": "7ff67690-de64-4942-97cf-30b18e8bfc0f"
      },
      "outputs": [
        {
          "ename": "OSError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_errors.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/pegasus-samsum-model/resolve/main/config.json",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    431\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, endpoint, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1345\u001b[0m             \u001b[0;31m# Repo not found => let's raise the actual error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1346\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mhead_call_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1347\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, endpoint, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1231\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1232\u001b[0;31m                 metadata = get_hf_file_metadata(\n\u001b[0m\u001b[1;32m   1233\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout)\u001b[0m\n\u001b[1;32m   1607\u001b[0m     )\n\u001b[0;32m-> 1608\u001b[0;31m     \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_errors.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    292\u001b[0m             )\n\u001b[0;32m--> 293\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRepositoryNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-6545f659-35bfd78844cdda270f994c85;4b11886a-ebc1-4798-bcd2-9be2eadb93d4)\n\nRepository Not Found for url: https://huggingface.co/pegasus-samsum-model/resolve/main/config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nInvalid username or password.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-d21a0ffcd201>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mgen_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"length_penalty\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"num_beams\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"max_length\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpipe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"summarization\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pegasus-samsum-model\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/__init__.py\u001b[0m in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    745\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mpretrained_model_name_or_path\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m             \u001b[0;31m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 747\u001b[0;31m             resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m    748\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m                 \u001b[0mCONFIG_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    449\u001b[0m         ) from e\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mRepositoryNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m         raise EnvironmentError(\n\u001b[0m\u001b[1;32m    452\u001b[0m             \u001b[0;34mf\"{path_or_repo_id} is not a local folder and is not a valid model identifier \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m             \u001b[0;34m\"listed on 'https://huggingface.co/models'\\nIf this is a private repository, make sure to pass a token \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: pegasus-samsum-model is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`"
          ]
        }
      ],
      "source": [
        "gen_kwargs = {\"length_penalty\": 0.8, \"num_beams\":8, \"max_length\": 128}\n",
        "\n",
        "pipe = pipeline(\"summarization\", model=\"pegasus-samsum-model\",tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IcZQgPhLPVGy"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(\"Dialogue:\")\n",
        "print(sample_text)\n",
        "\n",
        "\n",
        "print(\"\\nReference Summary:\")\n",
        "print(reference)\n",
        "\n",
        "\n",
        "print(\"\\nModel Summary:\")\n",
        "print(pipe(sample_text, **gen_kwargs)[0][\"summary_text\"])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0e422b86b5eb44e8b08a65f5f89330fc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10bc78c1dcc64bd7898911448067bc2b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d6170501b574013a935dc204eec2d8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10bc78c1dcc64bd7898911448067bc2b",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_af601fca2f6442deab5da73f1cdfae48",
            "value": " 819/819 [00:00&lt;00:00, 1071.94 examples/s]"
          }
        },
        "339580ead7ee4157b35050404a1409be": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_78a9f8ec97b04bb2b7248dd4f38958c5",
              "IPY_MODEL_d762ca4a6ffc442580dfafdff012c11e",
              "IPY_MODEL_2d6170501b574013a935dc204eec2d8a"
            ],
            "layout": "IPY_MODEL_68c9a8d24db24e6d9d992729e4118c71"
          }
        },
        "369272629e5049068464c417f2271cd4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68c9a8d24db24e6d9d992729e4118c71": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78a9f8ec97b04bb2b7248dd4f38958c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_369272629e5049068464c417f2271cd4",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_bcc690560e1744dba8288cc90be1984a",
            "value": "Map: 100%"
          }
        },
        "a155d65e3b364ad6a0589b1ac086ab46": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "af601fca2f6442deab5da73f1cdfae48": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bcc690560e1744dba8288cc90be1984a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d762ca4a6ffc442580dfafdff012c11e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e422b86b5eb44e8b08a65f5f89330fc",
            "max": 819,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a155d65e3b364ad6a0589b1ac086ab46",
            "value": 819
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}